# -*- coding: utf-8 -*-
"""Week 4 Introduction to Regression Independent Project- Emmanuel Odenyire.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WpE0Sv9lykcz4DWVrKwFWQw_v_PzW_Me

#Introduction to Regression Project

##Problem Statement

Mobile carrier Megaline has found out that many of their subscribers use legacy plans. They want to develop a model that would analyze subscribers' behavior and recommend one of Megaline's newer plans: Smart or Ultra. You have access to behavior data about subscribers who have already switched to the new plans (from the project for the Statistical Data Analysis course). For this classification task, you need to develop a model that will pick the right plan. Since you’ve already performed the data preprocessing step, you can move straight to creating the model. Develop a model with the highest possible accuracy. In this project, the threshold for accuracy is 0.75. Check the accuracy using the test dataset.

## Data Importation
"""

#Importing the necesary libraries
import pandas as pd
# Dataset URL (CSV File): https://bit.ly/UsersBehaviourTelco
#Reading the data using pd.read_csv
subscribers_df = pd.read_csv("https://bit.ly/UsersBehaviourTelco")

#Previewing the first 5 top records
subscribers_df.head()

"""##Data Exploration"""

#Getting the shape of the dataframe using .shape() gives us the number of rows and columns
subscribers_df.shape

#Describing the data using .info() function
subscribers_df.info()

# .describe() gives us the statistical description of the data
subscribers_df.describe()

#Looking at the amount of data on each target
subscribers_df['is_ultra'].value_counts()

# Those promoted and those not as a percentage
print('1. The percentage of mobile users on Smart plan are ' 
      + str(round(((subscribers_df["is_ultra"].isin([0]).sum())/subscribers_df.shape[0])*100,2)) + ' %')
print('2. The percentage of mobile users on Ultra plan are ' 
      + str(round(((subscribers_df["is_ultra"].isin([1]).sum())/subscribers_df.shape[0])*100,2)) + ' %')

"""##Data Cleaning

Data cleaning has already been done for this data set as per the instructions

*Since you’ve already performed the data preprocessing step, you can move straight to creating the model*

##Data Preparation
"""

#Checking the correlation of features and targets in the dataset
#Importing seaborn and matplotlib

import matplotlib.pyplot as plt
import seaborn as sns

features = subscribers_df.columns
corr_= subscribers_df[features].corr()
plt.figure(figsize=(6,4))
sns.heatmap(corr_, annot=True, fmt = ".2f", cmap = "BuPu");

# Plotting an Histogram for features to show relationship between features and target 
for feature in features[:-1]:
  plt.hist(subscribers_df[subscribers_df['is_ultra']==1][feature], color= 'green', alpha = 0.7, label = 'Ultra', density=True)
  plt.hist(subscribers_df[subscribers_df['is_ultra']==0][feature], color= 'red', alpha = 0.7, label = 'Smart', density=True)
  plt.title(feature)
  plt.ylabel('Probability')
  plt.xlabel(feature)
  plt.legend()
  plt.show()

# Splitting the source data into a training set, a validation set and a test set.
#First import train_test_split from sklearn
from sklearn.model_selection import train_test_split

features = subscribers_df.drop(['is_ultra'], axis=1)
target = subscribers_df['is_ultra']

# Setting aside 20% of train and test data for evaluation(Splitting into train and test data)
X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size=0.2, random_state = 54321)

# Using the same function above for the validation set
X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state= 54321) # 0.25 x 0.8 = 0.2

# Checking the shape of our new datasets

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"Y_train shape: {Y_train.shape}")
print(f"Y_test shape: {Y_test.shape}")
print(f"X_val shape: {Y_train.shape}")
print(f"Y_val shape: {Y_test.shape}")

"""##Data Modeling"""

# 3. Investigating the quality of different models by changing hyperparameters. 
# Briefly describe the findings of the study.

#import DecisionTreeRegressor and DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
# RandomForestRegressor and is located in sklearn.ensemble module.
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

dec_regressor = DecisionTreeClassifier(random_state=27)
log_regressor = LogisticRegression()
fst_regressor = RandomForestClassifier(random_state= 14)


dec_regressor.fit(X_train, Y_train)
log_regressor.fit(X_train, Y_train)
fst_regressor.fit(X_train, Y_train)

# Making Predictions using the validation set
dec_y_pred = dec_regressor.predict(X_val)
log_y_pred = log_regressor.predict(X_val)
fst_y_pred = fst_regressor.predict(X_val)

from sklearn.metrics import mean_squared_error
# Finally, evaluating our models 
print(f'Decision Tree RMSE: {mean_squared_error(Y_val, dec_y_pred, squared=False)}')
print(f'Linear Regression RMSE:{mean_squared_error(Y_val, log_y_pred, squared=False)}')
print(f'Random Forest Classifier RMSE: {mean_squared_error(Y_val, fst_y_pred, squared=False)}')

#Import Classification_report
from sklearn.metrics import classification_report
# print classification report for Decision Tree Regressor
print(f'DecisionTreeClassifier classification report:\n {classification_report(Y_test, dec_y_pred)}')

# Printing classification report for Logistic Regression
print(f'Logistic Regression classification report:\n {classification_report(Y_test, log_y_pred)}')

# Printing classification report for Random Forest Classifier 
print(f'Random Forest Classifier classification report:\n {classification_report(Y_test, fst_y_pred)}')

"""The model's perfomance accuracy given below:-

DecisionTreeClassifier 0.57

Logistic Regression 0.65

Random Forest Classifier 0.59

###Model Improvement with Hyperparameters
"""

##Finding the best tree depth with best accuracy
from sklearn.metrics import accuracy_score
best_score = 0
for depth in range(1,10):
  model = DecisionTreeClassifier(max_depth=depth, random_state=27)
  model.fit(X_test, Y_test)
  pred = model.predict(X_val)
  score = accuracy_score(Y_val, pred)
  if score > best_score: best_score = score
print(f'Tree accuracy with Validation: {best_score} at depth of: {depth}')

#Finding the best n estimator for random forest with best accuracy
best_score = 0
for n in range(1,20):
  model = RandomForestClassifier(n_estimators=n, random_state=12345)
  model.fit(X_train, Y_train)
  score = model.score(X_val, Y_val)
  if score > best_score: best_score = score
print(f'Forest accuracy with Validation: {best_score} for n trees: {n}')

# Retraining our models with hyper parameteres 
dec_regressor = DecisionTreeClassifier(random_state=27,max_depth = 9 )
log_regressor = LogisticRegression()
fst_regressor = RandomForestClassifier(random_state= 14, n_estimators = 19)

dec_regressor.fit(X_train, Y_train)
log_regressor.fit(X_train, Y_train)
fst_regressor.fit(X_train, Y_train)

"""##Model Evaluation"""

# 4. Checking the quality of the model using the test set.

# Making Predictions using the test set
dec_y_pred = dec_regressor.predict(X_test)
log_y_pred = log_regressor.predict(X_test)
fst_y_pred = fst_regressor.predict(X_test)


# Evaluating the models 
print(f'Decision Tree RMSE: {mean_squared_error(Y_test, dec_y_pred, squared=False)}')
print(f'Linear Regression RMSE:{mean_squared_error(Y_test, log_y_pred, squared=False)}')
print(f'Random Forest Classifier RMSE: {mean_squared_error(Y_test, fst_y_pred, squared=False)}')

# 5. Additional task: sanity check the model. This data is more complex than what
# you’re used to working with, so it's not an easy task. We'll take a closer look at it
# later.

from sklearn.metrics import classification_report
# Print classification report for Decision Tree Classifier 
print(f'DecisionTreeClassifier classification report:\n {classification_report(Y_test, dec_y_pred)}')

# Print classification report for Logistic Regression
print(f'Logistic Regression classification report:\n {classification_report(Y_test, log_y_pred)}')

# Print classification report for Random Forest Regressor
print(f'Random Forest Classifier classification report:\n {classification_report(Y_test, fst_y_pred)}')

"""##Findings and Recommendations

After hyperparamer tuning, the accuracy score on our test dataset improved as follows 

DecisionTreeClassifier 0.76 

Logistic Regression 0.67

Random Forest Classifier 0.77

The accuracy score of our models tells us that it is possible to predict the plan of customers as to wether they are on smart or ultra with s good amount of accuracy.
Random Forest and Decision Tree perfromed better in terms of accuracy
"""